---
title: "Exploratory Analysis"
subtitle: "Graphically assessing model assumptions and trends"
author: "Joe Watson and Marie Auger-Méthé"
date: "07/01/2021"
output: 
  html_document:
    css: "CSSdefs.css"
    after_body: footer.html
---

<script src="js/hideOutput.js"></script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Crucial to all good statistical analyses is a thorough exploratory analysis. In this document, we will introduce a few techniques for developing an understanding of our dataset, including an understanding of its limitations.

### Load packages

Let's load all the required R packages. See our tutorial *0_Installing_packages.html* for installation information.

```{r warning=FALSE, message=FALSE, error=FALSE}
library(rgeos)
library(rgdal)
library(sp)
library(maptools)
library(spatstat)
library(spdep)
library(INLA)
library(inlabru)
library(readxl)
library(lubridate)
library(ggmap)
library(raster)
```

### File set up

If you have not done so already, you need to download the two set of files associated with the workshop. 

All the workshop script and tutorial files can be found on Github at [github.com/joenomiddlename/DFO_SDM_Workshop_2020](https://github.com/joenomiddlename/DFO_SDM_Workshop_2020). To download these files on your computer, press on *Code* and then *Download ZIP*. Once downloaded, unzip the *DFO_SDM_Workshop_2020-main.zip* file. To be able to access some of the precompiled data, we need to make the unzipped folder the working directory. To do so in R Studio, navigate to the correct folder using the bottom right panel (folder view, you can use *...* button) and open it. Then, click "Set as Working Directory" under the tab *'More'*.  

We should be inside the folder titled: 'DFO_SDM_Workshop_2020', you can verify this by using `getwd()`.

Now we can load in the precompiled data
```{r message=FALSE}
list2env(readRDS('./Data/Compiled_Data_new.rds'), globalenv())
```

### Fin whale data

Throughout this workshop, we use data collected on fin whales in June, July, and August across the years 2007-2009 and for 2011. There are two main types of data:

* Sightings from systematic aerial surveys, found in `Sightings_survey`. The sightings information contain counts of fin whales, although for this tutorial we simplify the analysis and only model presence/absence. These systematic survey also contain information such as the distance of the sightings from the plane. Aerial survey sightings are accompanied with the aerial tracklines, found in `Effort_survey`. In these data objects, we combined three surveys conducted by both DFO and NOAA: 2007 T-NASS Aerial Survey (Lawson et al. 2009), NOAA NARW Surveys (Cole, NOAA), and NOAA Cetacean Surveys (Cole et al., NOAA). 

* Opportunistic fin whale sightings from two whale-watching operators, found in `Sightings_DRWW_sp`. This dataset is not accompanied by tracklines. Again, these datasets contains whale counts, but for simplicity we will analyze them as presence/absence data in these tutorials. These sightings are maintained in the DFO Maritimes Region Whale Sightings Database (MacDonald et. al. 2017). 

* **JOE: add info on Dist_Brier and Dist_Quody** 

* Ocean depth data, found in `Bathym`. Bathymetry data will be our main covariate in our analysis and we will explore whether the distribution of fin whales is linked to bathymetry.

### Coordinate reference system

These data are spatially-referenced data. The first thing we must always do is check for consistency between the coordinate reference systems (CRS) of each spatial object in use! To print the CRS of an *sp* object, simply add `@proj4string` to the name of the spatial object and run. 

```{r}
Sightings_DRWW_sp@proj4string 
Sightings_survey@proj4string 
Effort_survey@proj4string 
Domain@proj4string 
Bathym@proj4string
WW_ports@proj4string
Dist_Brier@proj4string
Dist_Quoddy@proj4string
```

All of the spatial objects are in lat/lon - good! For future analysis we will be projecting the data into a different coordinate reference system to better preserve euclidean distance. 

Finally, let's turn off all warnings associated with coordinate reference systems. The latest PROJ6+/GDAL3+ updates have caused many warning messages to be printed.
```{r}
rgdal::set_rgdal_show_exportToProj4_warnings(FALSE)
rgdal::set_thin_PROJ6_warnings(TRUE)
options("rgdal_show_exportToProj4_warnings"="none")
```


## Plotting the data

Our first goal is generally to plot the data on a map. The `gg()` and `gmap()` functions from the *inlabru* package is extremely useful at plotting spatial data! The class of spatial objects we will use are from the *sp* package. The classes of these objects begin with 'Spatial'. For example **Spatial**PointsDataFrame.

We have written a bespoke function `gg.spatiallines_mod()` to easily add SpatialLinesDataFrame objects to the plots too. This will prove useful for plotting transect lines. We load the bespoke functions to the working environment now.

```{r}
source('utility_functions.R')
```

Let's plot our data! 

If the data are in lat/lon format then the `gmap()` function will automatically add a terrain layer to the plots. Let's plot the survey sightings in blue, the survey tracklines in black, the whale-watch sightings in purple, the whale watch ports in red.

```{r 'map-gmap', message=FALSE, cache=TRUE, warning=F}
gmap(Sightings_survey) +
  gg(Domain) +
  gg.spatiallines_mod(Effort_survey) +
  gg(Sightings_survey, colour='blue') +
  gg(Sightings_DRWW_sp, colour='purple') +
  gg(WW_ports, colour='red')
```

Some of the maps used by default in `gmap` are copyrighted  (e.g., Google maps, see `?get_map`), see [Additional tips](#Ad1) for ways to use open-source maps for publication.

To remove the map layer, simply replace the `gmap(Sightings_survey)` with `ggplot()`.

```{r 'map-noMapLayer', message=FALSE, cache=TRUE}
ggplot() + # Notice the empty ggplot() call
  gg(Domain) +
  gg.spatiallines_mod(Effort_survey) +
  gg(Sightings_survey, colour='blue') +
  gg(Sightings_DRWW_sp, colour='purple') +
  gg(WW_ports, colour='red')
```

This plot hides some crucial information regarding the data collection. For example, the survey sightings and tracklines do not come from a single survey, or even a single organisation! Let's plot this! The easiest way to do this is to subset the data accordingly!

```{r 'map-survey', message=FALSE, cache=TRUE}
table(Effort_survey$DATASET)
# there are 3 surveys
ggplot() +
  gg(Domain) +
  gg.spatiallines_mod(Effort_survey[Effort_survey$DATASET=='DFO',], colour='purple') +
  gg.spatiallines_mod(Effort_survey[Effort_survey$DATASET=='NOAA_1',], colour='red') +
  gg.spatiallines_mod(Effort_survey[Effort_survey$DATASET=='NOAA_2',], colour='yellow')

```

This is problematic! The DFO tracklines (in purple) do not overlap with the two NOAA surveys! Thus, any future model will be unable to identify any differences in protocol efficiency. This is because any model intercepts will be confounded with the latent spatial field. More on this later!

### Exercise 1 {#Ex1}
In addition, the surveys were conducted across 4 separate years. Let's plot the survey tracklines by year. Do you see any cause for concern? Note that the names of the variables in the `Effort_Survey` are: `r names(Effort_survey)[1]` and `r names(Effort_survey)[2]`. Try this on your own! If you get stuck, click 'Show Code'. Hint: The YEAR variable is of type character and contains 4 unique values (see below).

```{r}
table(Effort_survey$YEAR)
class(Effort_survey$YEAR)
```

<div class="fold s o">
```{r 'map-years', tidy=FALSE, cache=TRUE}
ggplot() +
  gg(Domain) +
  gg.spatiallines_mod(Effort_survey[Effort_survey$YEAR=='2007',], colour='purple') +
  gg.spatiallines_mod(Effort_survey[Effort_survey$YEAR=='2008',], colour='red') +
  gg.spatiallines_mod(Effort_survey[Effort_survey$YEAR=='2009',], colour='blue') +
  gg.spatiallines_mod(Effort_survey[Effort_survey$YEAR=='2011',], colour='yellow')
```
</div>

Note that the surveys from years 2007, 2008 and 2009 covered largely different regions! Again, this is problematic if we want to model any changes in the whale distribution over time! The effect of year will be confounded by the spatial field. That being said, the data from 2011 appear to be a good candidate for model comparison as the spatial range overlaps with the other 3 years' effort. We will holdout this data as our test data and use 2007, 2008, and 2009 as our training data.

```{r}
xtabs(~ YEAR + DATASET, data=Effort_survey@data)
```
2011's data comes exclusively from NOAA.

## Transforming the data into a new CRS

For modelling, we will transform the data from lat/lon into a new "Canadian" CRS: NAD83 datum with UTM Zone 20N projection. This projection will help to preserve euclidean distance between points. We define the CRS object for this projection with EPSG code 2961:

```{r 'can_proj', cache=TRUE}
Can_proj <- CRS("+init=EPSG:2961")
Can_proj <- fm_crs_set_lengthunit(Can_proj, unit='km')
```

The second line of code specifies that we want to work in units of km instead of the default meters. This can prove vital in applications to avoid numerical overflow.

### Transforming spatial points, lines, and polygons

To do the transformation, we will use the `spTransform()` function. For example, we transform the whale-watch sightings spatial object `Sightings_DRWW_sp` as follow:

```{r 'ww_transform', warning=FALSE, cache=TRUE}
Sightings_DRWW_sp <- spTransform(Sightings_DRWW_sp, Can_proj)
Sightings_DRWW_sp@proj4string
```

Notice the changed output from calling `@proj4string`. 

### Exercise 2 {#Ex2}

Please repeat this for all the spatial objects that are points, lines or polygons.

<div class="fold s o"> 
```{r 'plp_transform', warning=FALSE, cache=TRUE}
Sightings_survey <- spTransform(Sightings_survey, Can_proj)
Effort_survey <- spTransform(Effort_survey, Can_proj)
WW_ports <- spTransform(WW_ports, Can_proj)
Domain <- spTransform(Domain, Can_proj)
```
</div>

### Transforming raster-like spatial pixels objects

Transforming the 'raster'-like SpatialPixelsDataFrame objects (`Bathym`, `Dist_Brier`, and `Dist_Quoddy`) using `spTransform` would be inappropriate here. The projection leads to a curvature of the pixels. A more appropriate approach here is to use bilinear interpolation. The `projectRaster()` function from the *raster* package works great for this. This requires converting the `SpatialPixelsDataFrame` object into an object of type `raster`. This is made easy with the function `raster()`. Finally, to convert the `raster` object back into a `SpatialPixelsDataFrame`, we can use the `as()` function from the *maptools* package. This function is extremely useful for converting spatial objects between the popular packages: *sp*, *spatstat*, and *sf*. We use this function substantially throughout the workshop.

```{r 'pixel_transform', warning=FALSE, cache=TRUE}
Bathym <- as(projectRaster(raster(Bathym), crs=Can_proj), 'SpatialPixelsDataFrame')
Dist_Brier <- as(projectRaster(raster(Dist_Brier), crs=Can_proj), 'SpatialPixelsDataFrame')
Dist_Quoddy <- as(projectRaster(raster(Dist_Quoddy), crs=Can_proj), 'SpatialPixelsDataFrame')
```

Plot the (transformed) Bathymetry and Distance from Port spatial objects. We are going to combine these into a single plot using the `multiplot()` function from the *inlabru* package. This function takes as input ggplot objects and an argument `layout`, specifying how the plots should be arranged (see [Additional tips](#Ad2) for ways to change the layout).

```{r 'map-multiplot', message=FALSE, cache=TRUE}
multiplot(ggplot() +
  gg(Domain) +
  gg(Bathym) + xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry') + 
    coord_fixed(ratio = 1),
ggplot() +
  gg(Domain) +
  gg(Dist_Brier) + xlab('East(km)') + ylab('North(km)') + 
  coord_fixed(ratio = 1),
ggplot() +
  gg(Domain) +
  gg(Dist_Quoddy) + xlab('East(km)') + ylab('North(km)')+ 
  coord_fixed(ratio = 1),
layout=matrix(1:4, nrow=2, ncol=2, byrow = TRUE))
```

Don't like the colour scheme? See [Additional tips](#Ad3) to learn how to define your own manually.

## Reducing the size of the domain (study area)

**JOE: as we talked about, this is too much detail for here. What I would prefer is that the domain saved in the data file (Compiled_Data_new.rds) and loaded at the top is the domain that looks like the intersect of the original domain and the restricted domain. And that you explain in detail how to change the domain for the creating the mesh and the covariates. I understand that this is annoying because the model fitting is hard, so if we can't get that, then fine. But we need to simplify this in some way without hiding the important parts. What I am asking is that if you change something else that would affect the model fit and will have to redo model fitting anyway, please do my suggestion. If not then, find a compromise solution. Talk to me about your solution first, so we decide together what's the best course of action before you do a lot of work.**

In its current form, the domain is too large for the application. The domain extends far beyond the survey tracklines and the whale-watch ports. To combat this, we will create a new domain that is restricted to lie within 30km of the nearest trackline.

```{r, cache=T, message=F}
# Create a set of SpatialPixels across the domain
pixels <- SpatialPoints(makegrid(Domain, n=5000),proj4string = Domain@proj4string)
# Convert the SpatialPixels into a SpatialPoints object before subsetting
#pred_int_points <- as(pred_int,'SpatialPoints')
# Subset to 30 km from Effort lines - can be slow
pixels_restricted <- 
  pixels[which(apply(gWithinDistance(pixels, Effort_survey,dist = 30,byid = T),2,sum)>0),]
# Select only the points that lie within the Domain
pixels_restricted <- pixels_restricted[Domain,]
# Convert to SpatialPolygons via SpatialPixels
pixels_restricted <- as(as(pixels_restricted,'SpatialPixels'),'SpatialPolygons')
# Merge the SpatialPolygons into a single Domain polygon using gUnionCascaded
pixels_restricted <- gUnionCascaded(pixels_restricted)
# Plot to check the new domain
ggplot() +
  gg(Bathym) + 
  gg(pixels_restricted) +
  xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry') + 
  coord_fixed(ratio = 1)
# Smooth the Domain using gSimplify
Domain_restricted <- gSimplify(pixels_restricted,tol=20)
# Plot
ggplot() +
  gg(Domain_restricted) +
  gg(Domain, color='red') +
  gg.spatiallines_mod(Effort_survey, colour='blue')
```

Whilst the new SpatialPolygons object `Domain_restricted` no longer extrapolates far beyond the survey tracklines, the smoothing achieved by the function `gSimplify()` has caused the new Domain to no longer contain all the tracklines. To counter this, we will buffer the study region prior to simplifying using `gBuffer`.

```{r}
Domain_restricted <- gSimplify(gBuffer(pixels_restricted,width=15),tol=20)
# Plot
ggplot() +
  gg(Domain_restricted) +
  gg(Domain, color='red') +
  gg.spatiallines_mod(Effort_survey, color='blue')
# Does the new domain contain all the survey tracklines?
gContains(Domain_restricted,Effort_survey)

# Create a new domain that is slightly larger - for defining covariates later
Domain_restricted2 <- gSimplify(gBuffer(pixels_restricted,width=40),tol=20)
# Plot
ggplot() +
  gg(Domain_restricted) +
  gg(Domain_restricted2, color='green') +
  gg(Domain, color='red')
# Does it contain the old 
gContains(Domain_restricted2,Domain_restricted)
```

Great! We have defined a new buffered and smoothed domain that will no longer extrapolate far beyond our survey tracklines. To restore the original coastline definition we can use the function `gIntersection()`. We now update the SpatialPolygons object `Domain` using `gIntersection()`:

```{r}
Domain <- gIntersection(Domain_restricted,Domain)
ggplot() + gg(Domain) + gg.spatiallines_mod(Effort_survey)
```


Note that we will need to use `Domain_restricted` for defining our computational mesh and `Domain_restricted2` for defining our covariates later in the workshop! The smoothing of the coastline will help us to define a computational mesh that has fewer numerical issues. More on that later!

## Extending the covariates to match the new domain

We will need to use `Domain_restricted2` for defining our computational mesh later in the workshop! The package `inlabru` requires that our covariates are also defined at every point in the new domain. This will require extending our covariates to take 'sensible' values at points beyond our original domain (`Domain`). We will see that these imputed values will not affect the model estimates. 

Our covariate depth is well-defined on the original (un-buffered) domain as defined by the SpatialPolygons `Domain`. At values outside this, we choose to 'fill-in' these points with nearest neighbour imputations. The code below does this for depth.

**JOE: see my comment below with regards to the bathymetry and removing values that are on land. Changing the values of the bathymetry (as in removing the pixels on land and explaining why log_depth and why log(1-log(depth)) was actually the thing I was suggesting to move here, not the restricting the covariate values to the mesh. I think the stuff associated with the mesh makes more sense in the next tutorial. What I'm envisioning is actually removing the values above 0 in the file that is save in the data file Compiled_Data_new.rds, make them NAs. And keep the explanation of log(1 - depth) here, since I think that's what you use in the Point process model below anyway and you can talk about why the transformation of the variable makes sense for the Point process model and for the other analysis in one go. Keep the crop to mesh size in tutorial 2 after you talk about the mesh. See the two comments below.**

```{r `map-covariatesWithMesh`, cache=TRUE}
## Redefine log_Depth across our modified domain
# Create the log depth and log slope covariates
log_Depth <- Bathym
log_Depth$FIWH_MAR_Bathymetry[log_Depth$FIWH_MAR_Bathymetry >= 0] <- 0
log_Depth$FIWH_MAR_Bathymetry <- log(1-log_Depth$FIWH_MAR_Bathymetry)
names(log_Depth) <- 'log_Depth' 

# 1) Define a set of pixels across our modified domain
pixels_Domain <- as(SpatialPoints(makegrid(Domain_restricted2, n=100000),proj4string = Domain@proj4string),'SpatialPixels')[Domain_restricted2,]

# 2) Extract values of the covariate at the new pixel locations
pixels_Domain$log_Depth <- over(pixels_Domain,log_Depth)$log_Depth
# 3) impute missing values with the nearest neighbour value
pixels_Domain$log_Depth[is.na(pixels_Domain$log_Depth)] <- 
  log_Depth$log_Depth[nncross(as(SpatialPoints(pixels_Domain@coords[which(is.na(pixels_Domain$log_Depth)),]),'ppp'),
                          as(SpatialPoints(log_Depth@coords),'ppp'),
                          what = 'which')]
log_Depth <- pixels_Domain
# 4) Create a squared log depth covariate (for later)
log_Depth_sq <- log_Depth
names(log_Depth_sq) <- 'log_Depth_sq'
log_Depth_sq$log_Depth_sq <- log_Depth_sq$log_Depth_sq^2

# Plot the covariates with Domain_restricted overlayed
ggplot() + gg(log_Depth) + gg(Domain_restricted)
```

We also need to buffer and rescale the distance from port covariates `Dist_Brier` and `Dist_Quoddy`.  Unlike with previous covariates, however, we will fix all buffered values on land equal to a large constant. This will help to ensure that negligible effort is recorded from the whale watch vessels on land through $\lambda_{eff}$.

We show how to do it with the distance to Brier.

```{r 'map-distBrierRescaled', message=FALSE, cache=TRUE}
# 1) Define a set of pixels across our modified domain
pixels_Domain <- as(SpatialPoints(makegrid(Domain_restricted2, n=100000),proj4string = Domain@proj4string),'SpatialPixels')[Domain_restricted2,]

# Extract the average value of distance at the newly created pixel locations
pixels_Domain$Dist_Brier <- over(pixels_Domain,Dist_Brier)$Dist_Brier
# There are some missing values due to newly created pixels being on land
# Fill in missing values with a very large value (1000km).
# This will make the effect of these regions negligible on inference as lambda_eff will be small
pixels_Domain$Dist_Brier[is.na(pixels_Domain$Dist_Brier)] <- 1e3

Dist_Brier <- pixels_Domain
ggplot() + gg(Dist_Brier) + gg(Domain)

# There is an infinity value at the port. Change to 0
Dist_Brier$Dist_Brier[is.infinite(Dist_Brier$Dist_Brier)] <- 0
max(Dist_Brier$Dist_Brier)

# Let's scale the Dist covariates closer to the (0,1) scale
Dist_Brier$Dist_Brier <- Dist_Brier$Dist_Brier / 980.7996
```

### Exercise {#ExNew}

Now, please rescale and buffer the distance to Quoddy object.

<div class="fold s o">
```{r 'map-distQuoddyRescaled', message=FALSE, cache=TRUE}
pixels_Domain <- as(SpatialPoints(makegrid(Domain_restricted2, n=100000),proj4string = Domain@proj4string),'SpatialPixels')[Domain_restricted2,]

pixels_Domain$Dist_Quoddy <- over(pixels_Domain,Dist_Quoddy)$Dist_Quoddy
pixels_Domain$Dist_Quoddy[is.na(pixels_Domain$Dist_Quoddy)] <- 1e3

Dist_Quoddy <- pixels_Domain
# There is an infinity value at the port. Change to 0
Dist_Quoddy$Dist_Quoddy[is.infinite(Dist_Quoddy$Dist_Quoddy)] <- 0
# Let's scale the Dist covariates closer to the (0,1) scale
Dist_Quoddy$Dist_Quoddy <- Dist_Quoddy$Dist_Quoddy / 980.7996
ggplot() + gg(Dist_Quoddy) + gg(Domain)
```
</div>

## Testing model assumptions and investigating trends

In the first lecture, we defined the species' intensity surface $\lambda_{true}(s)$. We interpreted this as an 'expected encounter rate per unit effort around s'. We then attempt to estimate/approximate an effort surface $\lambda_{eff}(s)$. Both of these surfaces are linked to a set of covariates (and later we'll even include random fields). 

For well-designed surveys, e.g. distance-sampling surveys, we have additional data available on the perpendicular distance of the encountered animal to the observer. We saw that we can use this to jointly model a detection probability function $p(d)$. *inlabru*, the package we will use in the later workshops, handles all of the necessary integrations required to jointly model the encounter locations with the encounter distances. 

Finally, we defined the observed intensity of the encounters $\lambda_{obs}(s)$ as 'the expected density of encounters per unit area around point s'. If our approximation of effort ($\lambda_{eff}(s)$), and the detection probability function $p(d)$ is good, then we hope to see: 

**JOE: the two definitions are not the most intuitive. Can you dumb them down and emphasize the differences? Something like lambda_obs is how many sightings (or the probability of a sighting?) we expect given the amount of effort in that area? And something like $\lambda_{true}(s)$ is the probability of a sighting at point s if it is searched. I'm sure some of this is not correct, but hopefully you get the idea.**

$$\lambda_{obs}(s) \approx \lambda_{true}(s) \lambda_{eff}(s)$$

or, for a **single** survey trackline: $$\lambda_{obs}(s) \approx \lambda_{true}(s) \lambda_{eff}(s) p(d(s))$$

**JOE: I'm not sure I understand why the p(d(s)) is only for a single trackline. Does it has to do with the single survey trackline? Or are you just introducing that p(d(s)) should be included sometimes? **

Later, we will be fitting spatial point process models to the encounters. A requirement for using these models is that, conditional on $\lambda_{eff}(s)$, each encounter with the animal is an **independent snapshot** from its true intensity $\lambda_{true}(\textbf{s})$.

Planned surveys can be designed to (approximately) satisfy the independence assumption. The boat speed can be fixed at a high value relative to the individuals' speeds (no double-counting) and a narrow perpendicular field-of-view can be enforced. Then, assuming that the individuals of the target species are in equilibrium with respect to their stationary distribution $\lambda_{true}(\textbf{s})$ (i.e. the species' density) and locally mixing faster than the time between revisits by observers, the independence assumption may reasonably hold.

Conversely, no such assumptions can be made regarding the whale-watch sightings. The unknown positions of the whale-watching boats **JOE: when they are not with whales? we have the location with whales, no?**, the 360 degree fields-of-view on-board, the variable boat speed, and the repeated sightings of the same individuals throughout each day can all invalidate the independence assumption required to fit the desired models.

In our workshop, we will see some potential solutions to address this issue of independence. In this session, we will perform some exploratory analyses to investigate the suitability of the independence assumption, and to investigate possible ways of estimating observer effort. 

### Investigating the autocorrelation between whale-watch sighting locations

Let's look at the whale-watching data. Specifically, let's investigate the 2nd, 3rd and 4th sightings in the dataset. Notice that they were made on the same day and very close together in time. Often, little information is provided on the database management procedures. For example, are these multiple encounters with the same individual? Would the database discard or retain problematic repeated sightings? 
```{r, warning=FALSE, message=F}
Sightings_DRWW_sp@data[2:4,c('WS_TIME','WS_DATE','PLATFORM')]
```

Without additional information available on the database management protocols, we can investigate whether these sightings could be of the same individual. To investigate this hypothesis, we can compute the distance between the sightings using the `gDistance()` function, the time between the sightings, and the required travel speed of the individual under the assumed hypothesis.

```{r}
# How far away are these sightings in space (in kilometers)?
gDistance(Sightings_DRWW_sp[2:4,], byid = T)
# How far away are these sightings in time (in minutes)?
Sightings_DRWW_sp[3:4,]$WS_TIME-Sightings_DRWW_sp[2:3,]$WS_TIME
# Could this be the same animal? How fast is this implied movement?
gDistance(Sightings_DRWW_sp[2:4,], byid = T)[cbind(c(1,2),c(2:3))] / 
  (c(18, 9)/60)
```

This hypothesis would imply a traveling speed of between 4km/h and 16km/h. Is this plausible? Since fin whales are known to sustain much higher speeds (> 30km/h), we will discard all repeated sightings each day. We will keep only the first sighting made each day by each company. 
Note that there is no overlap between the sighting locations from the two whale-watch companies, indicating that an assumption of independent encounters between the two companies could be reasonable. 

**JOE: given how fast fin whales move, either check this further (how distant apart are the encounters from different whale watch on the same day) or add a caveat/warning here about it.**

```{r 'map-overlap', message=FALSE, cache=TRUE}
Sightings_DRWW_sp$PLATFORM <- as.factor(Sightings_DRWW_sp$PLATFORM)
ggplot() + gg(Domain) + gg(Sightings_DRWW_sp, colour=Sightings_DRWW_sp$PLATFORM_CODE)
```

Thus we subset the data to keep only the initial sightings from each company:

```{r}
Sightings_Brier_nodup <- Sightings_DRWW_sp[Sightings_DRWW_sp$PLATFORM=='BRIER ISLAND WHALEWATCH',]
Sightings_Brier_nodup <- Sightings_Brier_nodup[!duplicated(Sightings_Brier_nodup$WS_DATE),]

Sightings_Quoddy_nodup <- Sightings_DRWW_sp[Sightings_DRWW_sp$PLATFORM=='QUODDY LINK',]
Sightings_Quoddy_nodup <- Sightings_Quoddy_nodup[!duplicated(Sightings_Quoddy_nodup$WS_DATE),]

dim(Sightings_DRWW_sp)[1]; dim(Sightings_Brier_nodup)[1]; dim(Sightings_Quoddy_nodup)[1];
```

Notice that we have a total of `r 144+85` whale watch sightings after the subsetting procedure, down from an original count of 567 sightings.

Strictly speaking, for the desired point process models to be reasonable, it is required that the initial daily encounter locations from the whale watch vessels are independent realizations from $\lambda_{true}$, conditioned on $\lambda_{eff}$. We will assume that the typical journeys/routes of the whale watch vessels remains constant across months and years under study. Thus, we assume that $\lambda_{eff}$ is constant through time. 

### Is summer species density changing through time?

As we discussed earlier, we are unable to model temporal changes in the species density due to the spatially non-overlapping survey efforts across the months and years. Thus any changes in the species intensity from June - August, or across the years 2007-2009 will be missed. Thus, we are forced to assume that the summer species density remains constant between 2007-2009. It is the summer species density that is our target for inference.

<!-- Note that it may be tempting to use the superposition property of point processes to justify the modelling of an 'combined s' -->

<!-- Superpositions of two **independent** LGCPS $X$ and $Y$ is itself a LGCP $Z$ with intensity $\lambda_Z$ equal to the sum of the intensities ($\lambda_X + \lambda_Y$). -->

How reasonable is our assumption that the species density remains constant across the years and across the months? Again, we can visually assess the suitability of the assumption with a plot.

If each whale-watch sighting is indeed an independent realization from a temporally static point process, then there should be no association between the spatial distances between the whale-watch sightings and how far apart in time they were made. 

To plot distances against time intervals, we first compute the distances in time and the euclidean distances in space between each sighting. 

```{r, message=FALSE}
difftime_Brier<- dist(as.numeric(ymd(Sightings_Brier_nodup$WS_DATE)))
diffspace_Brier <- dist(Sightings_Brier_nodup@coords)
difftime_Brier_fac <- as.factor(difftime_Brier)

difftime_Quoddy <- dist(as.numeric(ymd(Sightings_Quoddy_nodup$WS_DATE)))
diffspace_Quoddy <- dist(Sightings_Quoddy_nodup@coords)
difftime_Quoddy_fac <- as.factor(difftime_Quoddy)
```

We will create multiple plots with days on the x-axis and km on the y-axis. The first plots are restricted to only consider temporal differences of less than 85 days. We are plotting a sequence of boxplots to help detect patterns. 

```{r 'fig-difference', message=FALSE, cache=TRUE}
ggplot(data=data.frame(difftime = 
difftime_Brier_fac[as.numeric(difftime_Brier)<85],
                       diffspace = as.numeric(diffspace_Brier)[as.numeric(difftime_Brier)<85]),
       aes(y=diffspace, group=difftime)) +
  geom_boxplot() +
  xlab('difference in time (days)') +
  ylab('difference in space (km)')
```

The second plot is a different version that uses a loess smoother curve to try to help detect the same patterns.

```{r 'fig-differencel85', message=FALSE, cache=TRUE}
ggplot(data=data.frame(difftime = as.numeric(difftime_Brier)[as.numeric(difftime_Brier)<85],
                       diffspace = as.numeric(diffspace_Brier)[as.numeric(difftime_Brier)<85]),
       aes(x=difftime, y=diffspace)) +
  geom_point() + geom_smooth(method='loess') +
  xlab('difference in time (days)') +
  ylab('difference in space (km)')

```

These two plots indicate some moderate autocorrelation between the sightings, with encounters made ~1 month apart typically being around 50% further away than those made 24 hours apart. Notice that the trend plateaus after ~14 days. Perhaps this trend is due to the autocorrelation being caused exclusively by persistence in animal movement? Furthermore, the flat-line trend between day ~30 and day ~75 could be evidence in favour of an approximately constant density across the summer months. If the space use did indeed change dramatically across the months (e.g. due to migration), then we would expect to see a monotonically increasing trend over time.

The third plot investigates the space-time relationship across years.

```{r 'fig-differenceYears', message=FALSE, cache=TRUE}
ggplot(data=data.frame(difftime = as.numeric(difftime_Brier)[],
                       diffspace = as.numeric(diffspace_Brier)[]),
       aes(x=difftime, y=diffspace)) +
  geom_point() + geom_smooth(method='loess')+
  xlab('difference in time (days)') +
  ylab('difference in space (km)')
```

This third plot shows no evidence that the average spatial separation between whale watch sightings differs year-to-year. This supports our assumption that the spatial density can be assumed to be constant across the years, albeit within the (very) small spatial region visited by the whale-watch boats departing from Brier Island.

We now repeat the plots, but for the whalewatch boats that depart from Quoddy, starting with the loess smoother curve on the first 84 days. 

```{r 'fig-difference-quoddy', message=FALSE, cache=TRUE}
ggplot(data=data.frame(difftime = as.numeric(difftime_Quoddy)[as.numeric(difftime_Quoddy)<85],
                       diffspace = as.numeric(diffspace_Quoddy)[as.numeric(difftime_Quoddy)<85]),
       aes(x=difftime, y=diffspace)) +
  geom_point() + geom_smooth(method='loess')
```

In this plot, we see a large separation in the y-axis. This is caused by a cluster of sightings made way East of the port as seen in the map created above (right-most cluster of blue points). To stop this from impacting our estimate of trend, we restrict the spatial differences and times to be those within each of the two clusters of sightings.

```{r 'map-difftime', message=FALSE, cache=TRUE}
ggplot(data=data.frame(difftime = 
difftime_Quoddy_fac[as.numeric(difftime_Quoddy)<85 & as.numeric(diffspace_Quoddy)<50],
                       diffspace = as.numeric(diffspace_Quoddy)[as.numeric(difftime_Quoddy)<85 & as.numeric(diffspace_Quoddy)<50]),
       aes(y=diffspace, group=difftime)) +
  geom_boxplot() +
  xlab('difference in time (days)') +
  ylab('difference in space (km)')

ggplot(data=data.frame(difftime = as.numeric(difftime_Quoddy)[as.numeric(difftime_Quoddy)<85 & as.numeric(diffspace_Quoddy)<50],
                       diffspace = as.numeric(diffspace_Quoddy)[as.numeric(difftime_Quoddy)<85 & as.numeric(diffspace_Quoddy)<50]),
       aes(x=difftime, y=diffspace)) +
  geom_point() + geom_smooth(method='loess')
```

These two plots show some evidence of potential autocorrelation upto ~14 days. This could be due to persistence of animal movement, for example, due to a few whales remaining in the same area for a few days. 

As we see below, there is no evidence for across year differences.

```{r 'fig-differenceYears-quoddy', message=FALSE, cache=TRUE}
ggplot(data=data.frame(difftime = as.numeric(difftime_Quoddy)[as.numeric(diffspace_Quoddy)<50],
                       diffspace = as.numeric(diffspace_Quoddy)[as.numeric(diffspace_Quoddy)<50]),
       aes(x=difftime, y=diffspace)) +
  geom_point() + geom_smooth(method='loess')+
  xlab('difference in time (days)') +
  ylab('difference in space (km)')
```

We have seen some evidence that residual autocorrelations remain in the sightings data. This is despite subsetting the data to the initial daily sightings. The result of this may be overdispersion. We may be able to partially correct for this by using of a log-Gaussian Cox process (introduced in lecture 2). By including latent effects (e.g. spatial fields) in the model, we can hopefully capture some of this additional clustering. In fact, log-Gaussian Cox processes are always overdispersed relative to Poisson processes.

However, trying to determine which latent effects are really describing $\lambda_{true}$ and which are simply capturing the movement persistence is problematic. For example, if we falsely attribute too much variability and clustering to movement persistence, then we risk underestimating the uncertainty with our estimates of $\lambda_{true}$! This could lead to over-confident inference.

### Creating training and testing datasets

It is common for multiple competing models to be fit. Deciphering which model fits the data 'best' using information criterion alone (AIC, DIC, etc.,) can be problematic. Performing model comparisons using the same dataset as were used to fit the models and perform exploratory analysis can be problematic (overfitting can occur). An alternative approach is to test both the predictive accuracy and the estimated uncertainty on an unseen dataset.

We now subset the data into a training and test set. We choose the training set to contain all the sightings from 2007-2009. For the test data, we choose **all** the sightings from 2011. See [Additional tips](#Ad4) for an explanation of why the potential repeated sighting in 2011 between survey and whale-watch data means that the whale-watching data cannot be added to the training dataset. Note that we perform all remaining exploratory analysis on the training data from hereon out.

Below, we split the data into training and test data.

```{r 'divide-datasets', cache=TRUE}
Sightings_Brier_nodup_test <- Sightings_Brier_nodup[Sightings_Brier_nodup$YEAR==2011,]
Sightings_Brier_nodup <- Sightings_Brier_nodup[Sightings_Brier_nodup$YEAR!=2011,]
Sightings_Quoddy_nodup_test <- Sightings_Quoddy_nodup[Sightings_Quoddy_nodup$YEAR==2011,]
Sightings_Quoddy_nodup <- Sightings_Quoddy_nodup[Sightings_Quoddy_nodup$YEAR!=2011,]
Sightings_survey_test <- Sightings_survey[Sightings_survey$YEAR==2011,]
Sightings_survey <- Sightings_survey[Sightings_survey$YEAR!=2011,]
Effort_survey_test <- Effort_survey[Effort_survey$YEAR=='2011',]
Effort_survey <- Effort_survey[Effort_survey$YEAR!='2011',]

# How many survey sightings by year?
table(Sightings_survey$YEAR)
```

Next, we evaluate a crude estimate of the total amount of survey effort spent each year, as measured by the total trackline length. Using this, we then crudely estimate a 'catch per unit effort' (CPUE) measure by year.

```{r 'effort-aprox', cache=TRUE}
# How much survey effort (in trackline length) is there by year
by(gLength(Effort_survey,byid = T), Effort_survey$YEAR, sum)
# Crude estimate of relative `CPUE'
(table(Sightings_survey$YEAR) / 
       by(gLength(Effort_survey,byid = T), Effort_survey$YEAR, sum))/
  min(table(Sightings_survey$YEAR) / 
        by(gLength(Effort_survey,byid = T), Effort_survey$YEAR, sum))

# How many WW sightings by year
table(Sightings_Quoddy_nodup$YEAR)
table(Sightings_Brier_nodup$YEAR)
```

The 'CPUE' values are highly variable! The wild differences could be due to the small efforts in years 2008 and 2009 relative to 2007. Alternatively, the differences could actually reflect real differences in whale density across the three distinct regions visited each year! Without taking a model-based approach and controlling for differences in spatial effort, it is unclear how to combine the sightings from these three surveys.

As we will see in the next tutorial, by taking a model-based approach to inference, we will be able to combine the data from all three surveys to estimate the whale density across space. Unfortunately, due to the lack of spatial overlap between the surveys' efforts (tracklines), we are unable to account for any differences between the survey protocols within a model without strong prior information available. For example, an intercept added to the model for each survey would be confounded by the spatial field. From now on, we assume that each survey is equivalent.

## Investigating trends between the environmental covariates and whale density

We will investigate trends between the whale density and the covariate bathymetry. We use only the training data.

A quick first step is to evaluate whether the empirical densities of each covariate at the encounter locations (representing habitat used by the whales) differ from the empirical density across the domain (representing available habitat values). Such difference may indicate that the whales associated with a subset of the available habitat (or indicate differences in observer coverage).  

To extract the covariate values at each location, we will convert the SpatialPixelsDataFrame objects into objects of class `im` from the *spatstat* package. Then, after creating a `ppp` point pattern object containing the sightings locations from all sources, we can easily extract the values of each covariate at each point location^[Alternatively, we can use the function `over()` from the *sp* package. We choose to create `im` objects so that we can later use the `ppm` function from *spatstat* for exploratory analysis purposes. `ppm` fits Poisson process models, which can be equivalent to Maxent models.]. 

```{r 'extract-slopeAndBathy', message=FALSE, warning=FALSE, cache=TRUE}
# Convert covariates to im class
Bathym_im <- as(as(Bathym, 'SpatialGridDataFrame'),'im')
# Convert Observation locations to ppp object
All_obs_ppp <- ppp(x=c(Sightings_survey@coords[,1],
                       Sightings_Brier_nodup@coords[,1],
                       Sightings_Quoddy_nodup@coords[,1]),
                   y=c(Sightings_survey@coords[,2],
                       Sightings_Brier_nodup@coords[,2],
                       Sightings_Quoddy_nodup@coords[,2]),
                   marks=
                     as.factor(c(rep('survey',dim(Sightings_survey@coords)[1]),
                                 rep('Brier',dim(Sightings_Brier_nodup@coords)[1]),
                                 rep('Quoddy',dim(Sightings_Quoddy_nodup@coords)[1]))),
                   window = as(Domain, 'owin'))
# Notice how easy it is to extract the values of bathymetry at these locations!
Bathym_obs <- Bathym_im[All_obs_ppp]
```

**JOE: I wrote this comment first, but see the above comment on bathymetry. Here is another example were I would just change the data loaded. Make a new bathy object that has values greater to 0 as NA (or something like that). Save that object in Compiled_Data_new.rds. Doing these kinds of things as much as possible will just simplify the code (and text). Keep in mind that not everyone find it as easy as you to read code. You are a super duper expert, most of us are just dummies. See comment below.**

First, we plot the empirical densities of the Bathymetry at the sightings locations at throughout the domain (study area). Let's only consider values in water (max bathym is 313, so restrict to < 0).

```{r 'map-bathy', message=FALSE, warning=FALSE, cache=TRUE}
ggplot(data=data.frame(Bathym_obs = Bathym_obs),
       aes(x=Bathym_obs, colour="Sightings")) +
  geom_density() +
  geom_density(data=data.frame(Bathym_dom = as.numeric(Bathym_im$v[Bathym_im$v<0])),
               aes(x=Bathym_dom, colour='Domain')) + xlab('Bathymetry (m)') +
  scale_color_manual(name = "Density", values = c('Sightings' = 'black', 'Domain' = 'red'))
```

**JOE: this presentation of log(1-bathy) here is great and specify that this is done here for the Poisson point process, but also for all of the models explored afterward. Then save the log_bathym objects in Modelling_Data.rds (and maybe Modelling_Data2_New .rds?) so that they can be loaded in the next tutorials without going through these steps. I'm not sure whether you need to do the transformation for the obs also, here it's just the domain.**

We see a very skewed distribution of Bathymetry with a huge range of values. Leaving the covariate in this form risks wild predictions of intensity being made! A log transform is desired. We also flip the sign to make it interpretable as log depth (larger is deeper).

```{r 'log-bathym', cache=TRUE}
# Transforming the covariate
log_Bathym_im <- Bathym_im
log_Bathym_im[log_Bathym_im >= 0] <- -1
log_Bathym_im$v <- log(1-log_Bathym_im$v)
```

Let's plot the transformed data.

```{r 'map-bathyLog', message=FALSE, warning=FALSE, cache=TRUE}
ggplot(data=data.frame(Bathym_obs = log(1-Bathym_obs)),
       aes(x=Bathym_obs, colour="Sightings")) +
  geom_density() +
  geom_density(data=data.frame(Bathym_dom = as.numeric(log_Bathym_im$v)),
               aes(x=Bathym_dom, colour="Domain")) +
  geom_vline(xintercept = 0, colour='blue') + xlab('log Depth') +
  scale_color_manual(name = "Density", values = c('Sightings' = 'black', 'Domain' = 'red'))
```

Perhaps there is evidence of whales preferring areas of shallower waters? 

These plots may suggest that sightings locations are associated with regions with shallower waters compared what is available in the study area. However, these results are really investigating the associations between $\lambda_{obs}$ and the covariates and **not** the desired associations between $\lambda_{true}$ and the covariates. The associations may simply reflect the types of regions preferentially visited by observers! Remember - most encounters are made by the whale-watch vessels close to port!

**JOE: I've tried to dumb down some of the text, just check that I have not included errors.**

In lecture 1, we introduced the simplest distributional assumption on the number of sightings in a region. We assumed that the distribution of the number of encounters within any region $A$ was Poisson distributed, with mean equal to the integrated intensity surface: 

$$N(Y \cap A) \backsim Poisson(\int \lambda_{obs}(s) ds)$$.

Remember that $N(Y \cap A)$ represents the number of sightings in $A$. This equation represents a homogeneous Poisson process, where we assume that the probability of encountering a whale does not change through space (i.e., is homogeneous). 

We will see in lecture 2, that if we define a set of regions $A_i$ to be a grid that covers the domain $\Omega$, if we increase the number of cells the likelihood of the Poisson counts converges to the likelihood of the inhomogeneous Poisson process. **JOE: I'm confused by this statement, and I also wonder whether the main point to be made here about inhomogeneous Poisson process is that lambda_obs changes through space.**

To investigate trends between the species intensity and the covariate bathymetry, we fit an inhomogeneous Poisson point process models to the sightings data using the `ppm()` function from the **spatstat** package. These are similar to Maxent models. Ignoring effort for the time being, what does a Poisson point process model tell us?

```{r 'ppm', message=FALSE, cache=TRUE}
# Fitting the Poisson point process model
Ppm_1 <- ppm(All_obs_ppp~log_Bathym_im, # equation describing the relationship
                  data=list(log_Bathym_im=log_Bathym_im,
                            Domain=as(Domain, 'owin')), # transform domain in the format required for ppm
                  subset = Domain)
summary(Ppm_1)
```

Looking at the Ztest symbols (p < 0.05 indicated by at least one *) and estimates in the table of fitted trend coefficients, we find a significant association between $\lambda_{obs}$ and log depth and thus some evidence that the whales may be found more often in regions with shallower waters (negative estimate).

Next, we can attempt to make a crude adjustment for effort and see if these estimates change. We have already created a SpatialPixelsDataFrame object containing distance from the two whale-watch ports. Next, we can create a SpatialPixelsDataFrame containing the perpendicular distances to the nearest trackline. This fails to account for overlap in survey tracklines, but offers a reasonable first approach to crudely control for effort for the purposes of exploratory analysis.

```{r 'tracklinDistance', warning=FALSE, message=FALSE, cache=TRUE}
Dist_Surveylines <- as(Dist_Brier,'SpatialPoints')
Dist_Surveylines$Dist_surveylines <- as.numeric(apply(gDistance(Dist_Surveylines, 
                                                                Effort_survey,byid = T),2,min))
Dist_Surveylines <- as(Dist_Surveylines,'SpatialPixelsDataFrame')
```

We can then map the distance to the nearest trackline and plot the histogram of nearest distances. 

```{r 'map-dectectionDistance', warning=FALSE, message=FALSE, cache=TRUE}
colsc <- function(...) {
  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,"RdYlBu")),
                       limits = range(...))
}
# Map the distance through space
ggplot() + gg(Domain) + gg(Dist_Surveylines) + colsc(Dist_Surveylines@data[,1])
# What is the maximum detection distance?
ggplot(Sightings_survey@data, aes(x=DISTANCE)) + geom_histogram()
```

The histogram of the perpendicular distances from the aircraft shows an apparent maximum detection probability at a value of distance above 0! This is common for aerial surveys. Given the limit size of the survey data chosen for the workshop, and the potential numerical issues we may face fitting a complex detection probability function, we will fix all values less than 250m equal to 250m. This crude approach will help to make the histogram appear as a monotonically decreasing function of distance, satisfying the simplest class of detection probability functions. This shouldn't be done in practice! A 2-parameter detection probability function could instead be fit.

In addition, it is often advised in distance sampling applications to threshold our upper detection distances at a 'sensible' value. Let's plot our observed distances from the trackline. It looks like 2km could be a reasonable threshold value to choose. Let's set all values above 2km equal to 2km and scale the distances onto the km units scale. **This rescaling to units of km is crucial for numerical stability.** We define a variable 'distance' that contains the rescaled distances. Note that a few distances are missing. We impute these with the mean distance.

```{r, cache=T}
Sightings_survey@data$DISTANCE <- ifelse(Sightings_survey@data$DISTANCE>250,Sightings_survey@data$DISTANCE,250)

Sightings_survey@data$DISTANCE <- 
  ifelse(Sightings_survey$DISTANCE>2000 & !is.na(Sightings_survey$DISTANCE),
         2000,Sightings_survey$DISTANCE)

# needs renaming to match the formula argument
Sightings_survey$distance <- Sightings_survey$DISTANCE
# There are a couple of missing distances. Impute these with the mean
Sightings_survey$distance[is.na(Sightings_survey$distance)] <- mean(Sightings_survey$distance,na.rm=T)
Sightings_survey <- Sightings_survey[,-c(8)]
Sightings_survey$distance <- Sightings_survey$distance / 1000

ggplot(Sightings_survey@data, aes(x=distance)) + geom_histogram()

```


The histogram perhaps shows an initial steady decline in sightings with distance, before decaying more quickly as distance increases. Again, we need to remember that this plot is showing associations between distance $d$ and $\lambda_{obs}$ and **not** with $p(d)$.  To capture this shape, we will use a half-norm detection function in the Poisson point process model: $$p(d) = exp\left(\frac{-d^2}{2\sigma^2}\right)$$ This requires us to square the values of distance from the trackline.

Now, we investigate a potential functional form for the distance from port covariate. To do this, we plot a histogram of the distances from port at each of the whale watch sighting locations.

```{r 'plot-funcFormDist', cache=TRUE}
# Plot the sightings with distance from the port
hist(over(Sightings_Brier_nodup,Dist_Brier)$Dist_Brier, main='Histogram of the distance from port of the Brier sightings', xlab = 'Distance from port')

hist(over(Sightings_Quoddy_nodup,Dist_Quoddy)$Dist_Quoddy,breaks=20, main='Histogram of the distance from port of the Quoddy sightings', xlab = 'Distance from port')
```

For both ports, we detect a decreasing frequency of sightings made as the distance from port increases. This relationship is clearer for Brier sightings than for Quoddy sightings. Furthermore, the functional form of the effect does not appear to be an exponential decay. Based on the histograms, we choose to model the functional form as a half-normal function. More complicated functional forms (e.g. weibull or hazard functions) could be used.

On the log scale, this covariate can be estimated as a linear effect on the squared distance from port. Thus, we create SpatialPixelsDataFrame objects which store the squared distances from port.

## THE ABOVE HISTOGRAM SECTION WILL BE MOVED TO EXPLORATORY ANALYSIS

We assume the same functional relationship between each whale watch effort intensity $\lambda_{eff, i}$ and distance from their port location $s_i^*$. Here $i$ indicates the port and $s_i^*$ denotes the $i^{th}$ port location. We scale the half-norm functions by a unique constant $\lambda_i$, to capture the different number of vessels across the two ports: $$\lambda_{eff,i}(s) = \lambda_i exp\left(\frac{-||s-s_i^*||^2}{2\sigma_i^2} \right)$$.

```{r 'halfnorm', warning=FALSE, message=FALSE, cache=TRUE}
Dist_Surveylines$Dist_surveylines <- scale(Dist_Surveylines$Dist_surveylines^2)
Dist_Brier_sq <- as(Dist_Brier,'SpatialPixels')
Dist_Brier_sq$Dist_Brier_sq <- scale(Dist_Brier$Dist_Brier^2)
Dist_Quoddy_sq <- as(Dist_Quoddy,'SpatialPixels')
Dist_Quoddy_sq$Dist_Quoddy_sq <- scale(Dist_Quoddy$Dist_Quoddy^2)
```

Now we fit a Inhomogeneous Poisson point process model. As we are specifying the model on the log intensity, we need to include unique intercept parameters $\alpha_{0,i} : i \in \{1,2\}$ for the two whale watch port intensities ($\alpha_{0,i} = log \lambda_i$) and three unique parameters $\alpha_1$ and $\alpha_{1,i} : i \in \{1,2\}$ defining the shapes of the three half norm functions ($\alpha_{1,i} = \frac{-1}{2\sigma_i^2}$ and $\alpha_1 = \frac{-1}{2\sigma^2}$).

We must also inform spatstat about the source of each sighting, by fixing the value of its mark.

```{r 'ppm2', warning=FALSE, message=FALSE, cache=TRUE}
# All_obs_ppp$marks <- 
#   as.factor(c(rep('survey',dim(Sightings_survey@coords)[1]),
#             rep('Brier',dim(Sightings_Brier_nodup@coords)[1]),
#             rep('Quoddy',dim(Sightings_Quoddy_nodup@coords)[1])))

Ppm_2 <- ppm(All_obs_ppp~log_Bathym_im +
                    I(ifelse(marks=='Brier',1,0)):Dist_Brier_im +
                    I(ifelse(marks=='Quoddy',1,0)):Dist_Quoddy_im +
                    I(ifelse(marks=='survey',1,0)):Dist_Survey_im +
                    marks,
                  data=list(log_Bathym_im=log_Bathym_im,
                            Dist_Brier_im = as(as(Dist_Brier_sq,'SpatialGridDataFrame'),'im'),
                            Dist_Quoddy_im = as(as(Dist_Quoddy_sq,'SpatialGridDataFrame'),'im'),
                            Dist_Survey_im = as(as(Dist_Surveylines,'SpatialGridDataFrame'),'im'),
                            Domain=as(Domain, 'owin')),
                  subset = Domain)
coef(summary(Ppm_2))
```

With this rough attempt at controlling for heterogeneous effort, we see that log depth is found to be significantly associated with the whale density, with a positive coefficient. The negative effects of all three complicated expressions imply that a negative association between 'distance' and observed intensity is witnessed (agreeing with common sense).

We can plot the estimated 'true' whale intensity $\hat{\lambda}_{true}(s)$ predicted from this model:

```{r 'fig-ppm', cache=TRUE}
# remove the effects of observers for plotting
pred_coef <- coef(Ppm_2)
pred_coef[c(1,3,4,5,6,7)] <- 0
plot(predict.ppm(Ppm_2, ngrid=c(300,300),
             new.coef=pred_coef)[[1]],superimpose=F,
     hcl.colors(60, "YlOrRd", rev = TRUE))

```

Alternatively, we can also plot the fitted observed intensity surface $\lambda_{obs} =\lambda_{true} \lambda_{effort}$) for all observer types. We plot the observed intensity surfaces for both the survey and Quoddy-based whale watch vessels.

```{r 'fig-maxentFitted', cache=TRUE}
pred_coef <- coef(Ppm_2)
plot(log(predict(Ppm_2, ngrid=c(300,300),
             new.coef=pred_coef)[[3]]),superimpose=F,
     hcl.colors(60, "YlOrRd", rev = TRUE))
# Plot Quoddy
plot(log(predict(Ppm_2, ngrid=c(300,300),
             new.coef=pred_coef)[[2]]),superimpose=F,
     hcl.colors(60, "YlOrRd", rev = TRUE))
```

We see that the observed intensity, $\lambda_{obs}$ is dominated by the effort $\lambda_{eff}$. Despite this, we still detect significant effects of both log depth.

This exploratory analysis is useful at informing us about possible species-environment relationships to investigate in our futurer models. By playing around with different covariates and functional forms (e.g. quadratic effects), we can develop an understanding of which models to formally compare in the modelling stage.

Note that the above effort-correction approach is **not** the same as the one performed by *inlabru* later. It should only be used for exploratory purposes.

### Assessing the suitability of an inhomogeneous Poisson process

Unfortunately, by being in the inhomogeneous Poisson process class of point processes, the above models lack one crucial feature. They are unable to account for any additional spatial correlations/clustering that are frequently caused by unmeasured environmental covariates and biological processes acting behind the scenes. 

<!-- We can actually investigate the suitability of this inhomogeneous Poisson process assumption by plotting the (estimated) inhomogeneous K functions. Loosely speaking, the K function evaluated at a distance r defines a (scalar multiple of) the expected number of extra events that occur within a distance $r$ of a randomly chosen point (event).  -->

<!-- The K function for a fitted inhomogeneous Poisson processes can be approximated^[for homogeneous Poisson processes it takes the form $K(r) = \pi r^2$]. To do this, a weighted empirical K function is computed from the data. The weights help to rescale the function such that we would expect to see the relationship $K(r) = \pi r^2$ if the model fit (and assumptions) were good. The empirical K function counts the observed number of inter-point distances (in the observed data) that fall within a sequence of distances $r_1, r_2 ,..., r_m$. Plotting the (weighted) empirical and expected (true) K functions together allows for model assumptions to be assessed. -->

<!-- If the empirical K function falls 'significantly' above the expected curve at a distance $r$, then additional clustering has been detected in the data that cannot be explained by the inhomogeneous model alone. Conversely, if the empirical K function falls 'significantly' below the expected curve at a distance $r$, then additional repulsion or regularity has been detected in the data that cannot be explained by the inhomogeneous Poisson model alone. This clustering or repulsion may be caused by covariates or biological processes that have been missed! Alternatively, it may be caused by additional clustering due to animal peristence. Let's plot these two curves now! -->

<!-- ```{r 'fig-kFunctions', warning=FALSE, cache=TRUE} -->
<!-- plot(Kinhom(All_obs_ppp,lambda=Ppm_2,correction = 'best',normpower = 1)) -->
<!-- ``` -->

<!-- We see a large discrepancy between the true curve (in red) and the empirical curve in black. This is evidence to suggest the fitted model in its current form is inadequate. -->

This is why we turn to log-Gaussian Cox processes in the next tutorial. By assuming the intensity surface to be a realisation from a spatially-smooth (log-Gaussian) random field, we will be able to adjust our inference to take into account any additional spatial correlations and/or unmeasured covariates! By doing this, both parameter estimates and measures of uncertainty should be better adjusted to account for these autocorrelations. We will describe log-Gaussian Cox processes in depth in the next lecture.


## Exercises

If you got stuck on any of the exercises, then please feel free to try them again. Here are links to the problems:

1. [Plotting data by year](#Ex1)

2. [Transforming spatial points and lines](#Ex2)

### Bonus Exercises

1. Transform one of the spatial objects back into longitude latitude.
2. Transform one of the spatial objects into units of meters.
3. (More challenging) Investigate the relationship of distance from port with the two sources of whale watch data? What do you notice? What functional form could explain this type of behaviour? **(Hint: use the `over()` function to extract the values of a spatial covariate (of type SpatialPixelsDataFrame) at point locations (stored as a SpatialPointsDataFrame object). Remember to subset the data accordingly!)**

## Additional tips and code 

### Using OpenStreetMaps instead of Google Maps {#Ad1}

For publication, there can be issues regarding copyright of Google Maps. Using OpenStreetMap can help. To guarantee this simply add the following argument: `source='osm', force=TRUE` to `gmap()`. Double check the console that the maps are indeed being downloaded from stamen or osm. For brevity we have suppressed the messages.

```{r 'map-openStreet', message=FALSE, warning=FALSE, tidy=FALSE, cache=TRUE, eval=FALSE}
gmap(Sightings_survey, source='osm', force=TRUE) +
  gg(Domain) +
  gg.spatiallines_mod(Effort_survey) +
  gg(Sightings_survey, colour='blue') +
  gg(Sightings_DRWW_sp, colour='purple') +
  gg(WW_ports, colour='red')
```

Note for this to work it needs to be in the original project (lat/lon), not UTM.


The `multiplot()` function is a very flexible function that enables publication-quality figures to be made with relative ease. 

### Changing the layout of multiplot() {#Ad2}

To change the order of the plots you can change the argument `byrow=TRUE` to `byrow=FALSE`. 

```{r 'map-multiplotOrder', message=FALSE, cache=TRUE}
multiplot(ggplot() +
  gg(Domain) +
  gg(Bathym) + xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry'),
ggplot() +
  gg(Domain) +
  gg(Dist_Brier) + xlab('East(km)') + ylab('North(km)'),
ggplot() +
  gg(Domain) +
  gg(Dist_Quoddy) + xlab('East(km)') + ylab('North(km)'),
layout=matrix(1:4, nrow=2, ncol=2, byrow = FALSE))
```

We can also change the size of the figures by changing the matrix in the layout.

```{r 'map-multiplotSubfigures', message=FALSE, cache=TRUE}
multiplot(ggplot() +
  gg(Domain) +
  gg(Bathym) + xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry'),
ggplot() +
  gg(Domain) +
  gg(Dist_Brier) + xlab('East(km)') + ylab('North(km)'),
ggplot() +
  gg(Domain) +
  gg(Dist_Quoddy) + xlab('East(km)') + ylab('North(km)'),
layout=matrix(c(1,1,2,3), nrow=2, ncol=2, byrow = TRUE))
```

As you can see plots of different size stretches the maps, to keep the set projection we can use 
`coord_fixed(ratio = 1)`.

```{r 'map-multiplotProject', message=FALSE, cache=TRUE}
multiplot(ggplot() +
  gg(Domain) +
  gg(Bathym) + xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry') +
    coord_fixed(ratio = 1),
ggplot() +
  gg(Domain) +
  gg(Dist_Brier) + xlab('East(km)') + ylab('North(km)') + 
  coord_fixed(ratio = 1),
ggplot() +
  gg(Domain) +
  gg(Dist_Quoddy) + xlab('East(km)') + ylab('North(km)') + 
  coord_fixed(ratio = 1),
layout=matrix(c(1,1,2,3), nrow=2, ncol=2, byrow = TRUE))
```

### Define your own colour palette {#Ad3}

We can define the colour palette easily.

```{r}
colsc <- function(...) {
  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,"PuBuGn")),
                       limits = range(...))
}
```

Look at `?RColorBrewer::brewer.pal` to see what other colour palettes are available.

```{r 'map-multiplotColor1', message=FALSE, cache=TRUE}
multiplot(ggplot() +
  gg(Domain) +
  gg(Bathym) + xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry') +
  colsc(Bathym@data[,1]),
ggplot() +
  gg(Domain) +
  gg(Dist_Brier) + xlab('East(km)') + ylab('North(km)') +
  colsc(Dist_Brier@data[,1]),
ggplot() +
  gg(Domain) +
  gg(Dist_Quoddy) + xlab('East(km)') + ylab('North(km)') +
  colsc(Dist_Quoddy@data[,1]),
layout=matrix(1:4, nrow=2, ncol=2, byrow = T))
```

Have a go at creating your own colour palette function. Investigate the effects of changing both arguments to `brewer.pal`.

<div class="fold s o"> 
```{r 'map-multiplotColor2', message=FALSE, cache=TRUE}
colsc2 <- function(...){
  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(7,"Spectral")),
                       limits = range(...))
}
multiplot(ggplot() +
  gg(Domain) +
  gg(Bathym) + xlab('East(km)') + ylab('North(km)') + labs(fill='Bathymetry') +
  colsc2(Bathym@data[,1]),
ggplot() +
  gg(Domain) +
  gg(Dist_Brier) + xlab('East(km)') + ylab('North(km)') +
  colsc2(Dist_Brier@data[,1]),
ggplot() +
  gg(Domain) +
  gg(Dist_Quoddy) + xlab('East(km)') + ylab('North(km)') +
  colsc2(Dist_Quoddy@data[,1]),
layout=matrix(1:4, nrow=2, ncol=2, byrow = T))
```
</div>

### Potential repeated whale-watch and survey sightings in 2011 {#Ad4}

Could we not also put 2011's whale-watch data into the training set? Below is some code showing that we cannot. A whale is sighted by at least one whale watch company on every day in 2011 that a survey detects a whale. The plot below shows that these sightings could have been of the same animal. Even if they weren't, the tracklines in 2011 still visited areas that the whale watch vessels were present. Since our training and test datasets are required to be independent of each other, we must therefore also remove the 2011 whale watch sightings from the training data.

<div class="fold s o">
```{r 'map-2011', message=FALSE, cache=TRUE}
# what dates were survey sightings made on in 2011?
unique(Sightings_survey_test$DATE_LO[Sightings_survey_test$YEAR==2011])
# Are sightings by the WW vessels made on these dates?
sum(grepl(Sightings_survey_test$DATE_LO[Sightings_survey_test$YEAR==2011][1],
     x=c(Sightings_Quoddy_nodup_test$WS_DATE[Sightings_Quoddy_nodup_test$YEAR==2011],
         Sightings_Brier_nodup_test$WS_DATE[Sightings_Brier_nodup_test$YEAR==2011])))>0
sum(grepl(Sightings_survey_test$DATE_LO[Sightings_survey_test$YEAR==2011][2],
     x=c(Sightings_Quoddy_nodup_test$WS_DATE[Sightings_Quoddy_nodup_test$YEAR==2011],
         Sightings_Brier_nodup_test$WS_DATE[Sightings_Brier_nodup_test$YEAR==2011])))>0
sum(grepl(Sightings_survey_test$DATE_LO[Sightings_survey_test$YEAR==2011][3],
     x=c(Sightings_Quoddy_nodup_test$WS_DATE[Sightings_Quoddy_nodup_test$YEAR==2011],
         Sightings_Brier_nodup_test$WS_DATE[Sightings_Brier_nodup_test$YEAR==2011])))>0
# Could they be of the same animal? 
ggplot() + gg(Domain) + 
  gg(Sightings_survey_test[Sightings_survey_test$YEAR==2011,],colour='green') +
  gg.spatiallines_mod(Effort_survey_test[Effort_survey_test$YEAR==2011,],colour='yellow')

```
</div>


## Acknowledgements
The code for hiding the Rmd code chunks came from Martin Schmelzer, found [here](https://stackoverflow.com/questions/37755037/how-to-add-code-folding-to-output-chunks-in-rmarkdown-html-documents/37839683#37839683)


## References to Data Sources:

References/Sources for data sets:


DFO Maritimes Region Whale Sightings Database

MacDonald, D., Emery, P., Themelis, D., Smedbol, R.K., Harris, L.E., and McCurdy, Q. 2017. Marine mammal and pelagic animal sightings (Whalesightings) database: a users guide. Can. Tech. Rep. Fish. Aquat. Sci. 3244: v + 44 p.

NOAA NARW Surveys

Timothy V.N. Cole. National Oceanic Atmospheric Administration, National Marine Fisheries Service, Northeast Fisheries Science Center. 166 Water Street, Woods Hole, MA, USA


2007 TNASS DFO Aerial Survey

Lawson. J.W., and Gosselin, J.-F. 2009. Distribution and preliminary abundance estimates for cetaceans seen during Canada’s marine megafauna survey - A component of the 2007 TNASS. DFO Can. Sci. Advis. Sec. Res. Doc. 2009/031. vi + 28 p.


NOAA Cetacean Surveys

Timothy V.N. Cole and D. Palka. National Oceanic Atmospheric Administration, National Marine Fisheries Service, Northeast Fisheries Science Center. 166 Water Street, Woods Hole, MA, USA

